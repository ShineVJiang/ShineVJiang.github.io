[{"title":"每日知识点总结","slug":"每日知识点总结","date":"2020-05-27T12:00:00.000Z","updated":"2020-05-29T08:07:33.459Z","comments":true,"path":"2020/05/27/每日知识点总结/","link":"","permalink":"http://yoursite.com/2020/05/27/%E6%AF%8F%E6%97%A5%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"每日知识点总结 记录每天的学习内容，便于随时巩固。 温故而知新","text":"每日知识点总结 记录每天的学习内容，便于随时巩固。 温故而知新 2020.05.27正态分布+MLE+MAP 假设线性回归误差满足正太分布，则对样本做极大似然估计得到最小二乘法 又假设参数先验分布为正太分布，则对样本做极大后验估计得到Ridge 中心极限定理和正态分布 多个独立统计量的和的平均值满足正态分布 抛掷骰子，出现的点数虽为平均分布，但取N=100统计平均值，平均值满足正态分布 身高为多个因素作用的结果，假设有100个因素，即N=100，每个人的身高都可以看做100因素的平均值(平均影响)，满足正态分布 集成学习(boosting/bagging) boosting：串行，“从错误中汲取经验”，adaboosting的shortcoming是串行过程中上一次错分的样本，gradient boosting的shortcoming是串行过程中上一次的Gradient。总结gradient boosting 比较好的博文：梯度提升、Gradient Boosting bagging：并行，“投票决定”，各个基分类器的训练数据不能相同 2020.05.28全概率、条件概率、乘法公式 一切概率都是条件概率，假设D为全样本空间，则P(A) = P(A∩D)/P(D) 计算概率是要记得用韦恩图想象各样本空间 再看朴素贝叶斯 朴素贝叶斯垃圾邮件分类：其中贝叶斯公式中的每项各自的含义 再看生成式模型和判别式模型 假设X是观测值的集合，Y是预测值的集合 生成式模型对联合概率分布建模P(X,Y)，再计算P(Y|X) = P(X,Y)/P(X)，其中分子就是联合概率，分母可通过边缘概率求出，P(X) = ∑P(X,Y) = ∑P(X|Y)P(Y) 判别模型直接对条件概率进行建模P(Y|X) 激活函数的特点 Sigmoid和Tanh容易过拟合的原因，从函数导数来看，提供的有效激活范围小 ReLU的优点：相对宽的激活边界使得不容易梯度消失、单侧抑制模拟大脑神经元、正反向传播计算复杂度低 ReLU的缺点：lr过大导致神经元死亡，新提出的LeakyReLU进行补救 2020.05.29LATEX 学习 最常用的键’\\\\’，用于将字符串构造成特殊符号，例如：\\gamma \\theta \\mbox \\gamma + \\theta + \\alpha + \\pi + \\mbox for Array相关：\\left(\\begin{array}{cc}\\end{array}\\right) \\left(\\begin{array}{cc} 1&2&3 \\\\ 4&5&6 \\end{array}\\right) 上下标和求和连乘等： X_{1}^{1}\\\\ \\sum_{x=1}^{n}x_i \\\\ \\prod_{x=1}^{n}y_i 概率论：随机试验、随机/基本事件、样本空间等 求概率时，一定要明确总的样本空间和事件的样本空间 随机事件由基本事件组合而成，随机事件的样本空间是总样本空间的子集 C++ ：引用、指针(C++ Primer P50) 引用：引用及别名，只能为对象创建引用，为引用赋值相当于为对象赋值，无法另引用重新绑定到新的值 指针：存放某个对象的地址 相同点：与引用类似，指针也实现了对其他对象的间接访问；需要类型匹配 不同点1：指针本身就是一个对象，允许对指针赋值和拷贝，生命周期内可以先后指向几个不同的对象 不同点2：指针无需在定义是赋值，与其他内置类型一样，在作用域内灭有初始化的指针拥有不确定的值 使用解引用符* 访问引用指向的对象的值","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"},{"name":"总结","slug":"总结","permalink":"http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)","slug":"“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)","date":"2020-05-21T13:00:00.000Z","updated":"2020-05-22T01:46:54.553Z","comments":true,"path":"2020/05/21/“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)/","link":"","permalink":"http://yoursite.com/2020/05/21/%E2%80%9C%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E2%80%9D%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E4%BA%8C)%EF%BC%9A%E5%AF%BC%E5%90%91%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD(Guided-Backpropagation)/","excerpt":"Striving for Simplicity: The All Convolutional Net 1. 简介上一篇“直观理解”卷积神经网络(一)：反卷积(Deconvnet)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/)提到：一个典型的卷积神经网络主要由卷积操作、非线性(ReLU)、池化操作等构成。很快，Jost Tobias Springenberg 等人开始质疑池化操作的必要性，并提出了不包含池化操作的“全卷积网络”和对应的新可视化方法“导向反向传播”。","text":"Striving for Simplicity: The All Convolutional Net 1. 简介上一篇“直观理解”卷积神经网络(一)：反卷积(Deconvnet)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/)提到：一个典型的卷积神经网络主要由卷积操作、非线性(ReLU)、池化操作等构成。很快，Jost Tobias Springenberg 等人开始质疑池化操作的必要性，并提出了不包含池化操作的“全卷积网络”和对应的新可视化方法“导向反向传播”。 2. 全卷积网络想要质疑并去除池化操作，就需要先明白池化操作的作用。一般认为:池化操作通过成倍的缩小feature maps的尺寸(高、宽)，使得一个特征可覆盖的原图的范围(感受野Receptive Field)成倍的增大，这样高层的特征可以更快地表征图片的宏观信息。而将卷积操作的步幅(stride)设置为大于1的值，理论上可以获得与池化操作相同的效果；论文作者也通过CIFAR-10、CIFAR-100、ImageNet数据集上的不弱的分类性能 进一步说明了实际的可行性(感兴趣的朋友可以在原论文查看具体的网络结构和对比实验)。下图中，卷积操作步幅为2。(图片来源：附录2) 3. 导向反向传播上一节提到的全卷积网络确实在公开数据集上取得了不俗的分类性能，但使用步幅为2的卷积替换掉池化 究竟对网络带来了怎样的影响呢？用我们上一篇文章提到的“反卷积”将 特征可视化看看吧，并没有达到预期效果，具体表现为：低层特征的可视化结果可以反应出特征学习到的内容，但高层特征的可视化结果不是清晰的、可辨认的结构(见下图最左侧)。还记得反卷积网络中反池化操作时利用的池化索引吗，这些索引记录了最大池化时最大值所在的位置。由于不同图片的池化索引各不相同，这样，即使是相同位置的特征通过反卷积网络后，可视化结果也完全不同(池化索引相当于给了不同图片独特的身份标识)。去掉池化层之后，图片的身份标识没有了，自然无法生成与原图对应的可视化结构。 从原理上应该这样理解：低层特征的计算复杂度很小，对应原图的区域很小，仅仅学到了极小的不变性，因此最大激活低层特征的是单一的、固定的模式；而高层特征的计算复杂度很大，对应原图的区域很大，学到了极大地不变性表示，因此不存在单一图片中的单一模式最大化激活高层特征。合理的特征可视化应该针对一张输入图片进行，对丢失了图片身份标识的全卷积网络，使用反卷积网络进行可视化，显然无法取得很好的效果。 还有什么办法可以衡量 图片的哪些部分最大化激活了一个特征吗？有的，那就是通过反向传播，计算该特征值相对于该输入图片的梯度。反向传播和反卷积最大的不同，就是数值通过通过ReLU时计算方式的不同。而导向反向传播可视化方法，上述两者的结合，细节见下图：b中的四行分别表示，经过ReLU时，前向传播、反向传播、反卷积、导向反向传播的不同之处：定义左侧为前向传播的输入列，右侧为前向传播的输出列，前向传播只向后传输入列大于0的值、反向传播只向前传梯度大于0(输入列大于零)的值、反卷积只向前传输出列大于0的值、导向反向传播综合前两者，只向前传输入列和输出列都大于0的值；c中的公式表达更为清晰。 导向反向传播(Guided-Backpropagation)相当于对普通的反向传播加了指导，限制了小于0的梯度的回传，而梯度小于0的部分对应了原图中 削弱了我们想要可视化的特征 的部分，这些部分正是我们不想要的。还记得我们的目标吗，找出最大化激活某个特征的图片部分。有趣的是，导向反向传播在全卷积网络中也表现非常好，上图是模型高层特征(layer6/layer9)的反卷积(deconv)、导向反向传播(Guided-Backpropagation)可视化结果对比。 半小时学会Pytorch Hook一文中通过pytorch模型中各层的hook函数，改变反向传播时对梯度的操作，用少量代码实现了导向反向传播。 附录 Striving for Simplicity: The All Convolutional Net A guide to convolution arithmetic for deep learning","categories":[],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yoursite.com/tags/Neural-Network/"},{"name":"Visualization","slug":"Visualization","permalink":"http://yoursite.com/tags/Visualization/"},{"name":"神经网络可视化","slug":"神经网络可视化","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"导向反向传播","slug":"导向反向传播","permalink":"http://yoursite.com/tags/%E5%AF%BC%E5%90%91%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"}]},{"title":"“直观理解”卷积神经网络(一)：反卷积(Deconvnet)","slug":"“直观理解”卷积神经网络(一)：反卷积(Deconvnet)","date":"2020-05-14T10:00:00.000Z","updated":"2020-05-22T01:47:09.285Z","comments":true,"path":"2020/05/14/“直观理解”卷积神经网络(一)：反卷积(Deconvnet)/","link":"","permalink":"http://yoursite.com/2020/05/14/%E2%80%9C%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E2%80%9D%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E4%B8%80)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/","excerpt":"“Visualizing and Understanding Convolutional Networks” — Matthew D.Zeiler 1. 简介时间倒退回2014年，大量基于卷积神经网络的模型在ImageNet数据集上的分类性能有了巨大的提升，其中翘楚当属AlexNet，凭借16.4%的准确率拿下2012年冠军，远远领先第二名的26%。但大多数人并没有完全理解这些模型表现为何如此之好，模型的改善也就无从谈起。对于模型的直观理解迫在眉睫，而本文提到的反卷积技术(Matthew D.Zeiler)就是该领域的开山之作：该技术尝试在输入图片的像素空间中找出最大化激活某一特征的像素，实现了对应特征的可视化，其中反卷积的过程就是寻找像素的过程。借助反卷积，作者也探索了“模型训练过程中，特征的演化”、“分类结果相对于图片遮挡部位的敏感性”、“特征不变性”和“图片相关性”等问题。","text":"“Visualizing and Understanding Convolutional Networks” — Matthew D.Zeiler 1. 简介时间倒退回2014年，大量基于卷积神经网络的模型在ImageNet数据集上的分类性能有了巨大的提升，其中翘楚当属AlexNet，凭借16.4%的准确率拿下2012年冠军，远远领先第二名的26%。但大多数人并没有完全理解这些模型表现为何如此之好，模型的改善也就无从谈起。对于模型的直观理解迫在眉睫，而本文提到的反卷积技术(Matthew D.Zeiler)就是该领域的开山之作：该技术尝试在输入图片的像素空间中找出最大化激活某一特征的像素，实现了对应特征的可视化，其中反卷积的过程就是寻找像素的过程。借助反卷积，作者也探索了“模型训练过程中，特征的演化”、“分类结果相对于图片遮挡部位的敏感性”、“特征不变性”和“图片相关性”等问题。 2. 卷积网络和反卷积网络2.1 卷积网络一个典型的卷积神经网络模型由一系列层(layer)构成，将一张RGB图像x转化为一个向量y_hat，向量中的C个值对应了C类的可能性。当时，卷积网络还很单纯(没有shortcut connection、没有DW、PW Conv等)，每一层都由如下构成： 卷积操作：采用一组训练得到的卷积核对上一层的输出作卷积。 增加非线性：让卷积结果通过非线性函数，例如relu(x) = max(x, 0)。 池化操作[可选]：取相邻元素的最大值(MaxPooling)或平均值(AveragePooling)，进行下采样；MaxPooling中池化索引记录了最大值对应的位置。最后添加全连接层和Softmax分类器，输出C个类别的可能性。 2.2 反卷积网络为了理解卷积网络，需要理解中间层的特征值，反卷积网络提供了办法。反卷积网络的每一层都可以看做卷积网络中对应层的逆过程，它们拥有相同的卷积核和池化索引，因此反卷积将特征值逆映射回了输入图片的像素空间，借此说明图片中的哪些像素参与激活了该特征值。下图将卷积网络和反卷积网络的过程合并，展示了两者各层之间的关系，且两者在整体上互为逆过程：首先，卷积网络将一张图片作为输入，计算得到各层的特征表示；为了验证某层一个具体的特征值，我们将该层特征值之外的所有值置零后，将其作为反卷积网络的输入，经过反卷积网络每一层的操作，该特征值被映射回了输入图片的像素空间. 反卷积网络由如下部分构成： 反池化操作[对应卷积网络池化操作]：理论上，卷积网络中的最大池化操作是不可逆的，但我们可以通过池化索引进行近似可逆。下图(图片来源：附录3)是反池化过程： 矫正[对应卷积网络ReLU操作]：卷积网络中采用ReLU确保特征值非负，为了确保正逆过程的一致性，我们将反卷积网络每一层的重构特征也通过ReLU得到非负值。 反卷积操作[对应卷积网络卷积操作]：卷积操作是低效操作，主流神经网络框架都是通过im2col+矩阵乘法实现卷积，以空间换效率。输入中每个卷积窗口内的元素被拉直成为单独一列，这样输入就被转换为了H_out * W_out列的矩阵(Columns)，im2col由此得名；将卷积核也拉成一列后(Kernel)，左乘输入矩阵，得到卷积结果(Output)。im2col和矩阵乘法见如下两图(图片来源：附录1)。 本文中提到的反卷积操作其实是转置卷积，神经网络框架借助转置卷积实现梯度的反向传播：如下两图(图片来源：附录1)：将卷积核矩阵转置(Weight_T)后，左乘梯度输出(GradOutput)，得到梯度列矩阵(GradColumns)，通过col2im还原到输入大小，便得到了相对于输入的梯度(GradInput)。 梯度的反向传播一般用于卷积网络的训练过程，通过梯度更新权重和偏正取值；而本文转置卷积不是用于梯度反向传播，操作对象不是梯度而是特征值。 3.卷积网络可视化使用上述的理论框架，我们就可以通过反卷积网络来“直观理解”特征值。 3.1 特征可视化下图展示了训练完成后，各层特征值的可视化结果。对于某一层某一个特定位置的特征，选取了9个最大的激活值(由9张输入图片经过卷积网络生成)，并分别将其映射回输入图片的像素空间，可视化结果的旁边是特征对应的可视野范围的输入图片截图。从Layer1 Feature Maps中选了9个位置的特征值，每个特征值选了最能激活该值的9张图片；从Layer2 Feature Maps中选了16个位置的特征值，每个特征值选了最能激活该值的9张图片。几点启示： 原图的截图比可视化结果的多样性更大，因为后者仅仅关注了截图中有判别能力的结构。举例来说：层5第1行第2列的9张输入原图差异很大，但同一特征值的可视化结果都关注了背景中的草地。 每层特征的可视化结构展示了各层的特点。层2展示了物体的轮廓和颜色的组合，每个特征的可视化结果大致相同(较小的不变性)；层3有了更复杂的不变性，主要是相似的纹理(1行1列的网格特征、2行4列的文本特征)；层4的不同特征对应的可视化结果有显著的差别，开始展现类与类之间的差异(1行1列都是狗狗的脸，4行2列都是鸟的腿)；层5的不同特征对应的可视化结果对应了类别间的物体的整体差异(不再是局部)(1行1列键盘，4行的狗)。因为可视野相应的变大了。 每个特征值对应的9个可视化结果有很强的关联性。 层次越高，不变性越强。 可视化结果都是原图中辨识度最强的部分的夸张展现。 3.2 训练过程中特征的演化在每一层随机选取6个特征，在训练过程中的第[1,2,5,10,20,30,40,64]个epoch时，将样本中最强激活的特征映射回输入图片像素空间。可以看出：底层特征很快可以趋于稳定，但高层特征需要更多次的迭代才能收敛，证明了迭代次数的必要性。 3.3 特征不变性下图第1列是5张示例图片的三种变换，分别为平移、缩放、旋转；列2和列3是层1和层7变换图和原图间的特征向量的欧几里得距离；列4是对应图片分类可能性的变化。总体来说：微小的变换对于低层的特征有显著地影响，而层越高，平移和缩放对结果的影响越小；但卷积网络无法对旋转操作产生不变性，除非物体有很强的不对称性。 3.4 遮挡敏感性当模型的分类性能提高时，一个自然而然的想法就是：分类器究竟基于什么信息做出了判断？是基于图片中的物体，还是图片中与物体无关的上下文信息。基于错误的信息做出判断就会出现著名的“坦克分类问题”。下图采用灰色矩形遮挡输入图片的不同部分，并将遮挡后的图片输入网络得到分类器的输出，1~3行分别是博美犬、车轮和阿富汗猎犬；a列是原图，b列是遮挡不同部位后得到的第五层的热力图(选取了不遮挡图片最大激活值所在的通道)，c列是不遮挡原图第5层最大激活值的可视化结果，其余3张来自别的输入图片，d列是遮挡不同部位后，正确分类可能性的热力图，e列是遮挡不同部位后，最可能的分类结果。总体来说： b列说明：当遮挡了可视化结果对应的原图部位时，对应的特征值会急剧下降； d列说明：遮挡博美犬脸的部位、车轮部位、阿富汗犬部位，正确分类的可能性急剧下降。 附录：1.PyTorch源码浅析(3)：NN 2Visualizing and Understanding Convolutional Networks 3.反卷积(Deconvolution)、上采样(UNSampling)与上池化(UnPooling)","categories":[],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yoursite.com/tags/Neural-Network/"},{"name":"Visualization","slug":"Visualization","permalink":"http://yoursite.com/tags/Visualization/"},{"name":"反卷积","slug":"反卷积","permalink":"http://yoursite.com/tags/%E5%8F%8D%E5%8D%B7%E7%A7%AF/"},{"name":"神经网络可视化","slug":"神经网络可视化","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"}]}]