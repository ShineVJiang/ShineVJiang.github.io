[{"title":"一文搞懂熵(Entropy),交叉熵(Cross-Entropy)","slug":"一文搞懂熵(Entropy),交叉熵(Cross-Entropy)","date":"2020-06-16T13:00:00.000Z","updated":"2020-06-17T15:55:35.634Z","comments":true,"path":"2020/06/16/一文搞懂熵(Entropy),交叉熵(Cross-Entropy)/","link":"","permalink":"http://yoursite.com/2020/06/16/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E7%86%B5(Entropy),%E4%BA%A4%E5%8F%89%E7%86%B5(Cross-Entropy)/","excerpt":"本文整理/翻译自Medium的系列文章Demystifying Entropy, Demystifying Cross-Entropy, Demystifying KL Divergence 1. 熵(Entropy)1.1 混乱程度，不确定性，还是信息量?不同的人对熵有不同的解释：混乱程度，不确定性，惊奇程度，不可预测性，信息量等等，面对如此多的解释，第一次接触时难免困惑。本文第一部分，让我们先一起搞明白 熵 究竟是什么？ 熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。","text":"本文整理/翻译自Medium的系列文章Demystifying Entropy, Demystifying Cross-Entropy, Demystifying KL Divergence 1. 熵(Entropy)1.1 混乱程度，不确定性，还是信息量?不同的人对熵有不同的解释：混乱程度，不确定性，惊奇程度，不可预测性，信息量等等，面对如此多的解释，第一次接触时难免困惑。本文第一部分，让我们先一起搞明白 熵 究竟是什么？ 熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。 1.2 计算编码长度上文提到了熵的定义：无损编码事件信息的最小平均编码长度。编码长度容易理解，但何来的最小，又何来的平均呢？下面以一个例子来说明：假设我们采用二进制编码东京的天气信息，并传输至纽约，其中东京的天气状态有4种可能，对应的概率如下图，每个可能性需要1个编码，东京的天气共需要4个编码。让我们采用3种编码方式，并对比下编码长度。不难发现，方式3编码长度最小，且是平均意义上的最小。方式3胜出的原因在于：对高可能性事件(Fine,Cloudy)用短编码，对低可能性事件(Rainy,Snow)用长编码。表中的3种方式，像是尝试的过程，那么能否直接计算出服从某一概率分布的事件的最小平均编码长度呢？还句话说，能不能直接计算熵？ 编码方式 Fine(50%) Cloudy(25%) Rainy(12.5%) Snow(12.5%) 编码长度 方式1 10 110 0 111 2✖50%+3✖25%+1✖12.5%+3✖12.5%=2.25 方式2 0 110 10 111 1✖50%+3✖25%+2✖12.5%+3✖12.5%=1.875 方式3 0 10 110 111 1✖50%+2✖25%+3✖12.5%+3✖12.5%=1.75 1.3 直接计算熵假设一个信息事件有8种可能的状态，且各状态等可能性，即可能性都是12.5%=1/8。我们需要多少位来编码8个值呢？1位可以编码2个值(0或1)，2位可以编码2✖2=4个值(00,01,10,11)，则8个值需要3位，2✖2✖2=8(000,001,010,011,100,101,110,111)。 A(12.5%) B(12.5%) C(12.5%) D(12.5%) E(12.5%) F(12.5%) G(12.5%) H(12.5%) 000 001 010 011 100 101 110 111 我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。归纳来看，对于具有N种等可能性状态的信息，每种状态的可能性P = 1/N，编码该信息所需的最小编码长度为： \\log_2{N} = - \\log_2{\\frac{1}{N}} = -\\log_2{P}那么计算平均最小长度，也就是熵的公式为： Entropy = -\\sum_{i}P(i)\\log_2{P(i)}其中P(i)是第i个信息状态的可能性。 回头看看上述编码东京天气的例子，熵 = 1 50% + 2 25% + 3 12.5% + 3 12.5% = 1.75 Fine(50%) Cloudy(25%) Rainy(12.5%) Snow(12.5%) 0 10 110 111 -log(50%) = 1 -log(25%) = 2 -log(12.5%) = 3 -log(12.5%) = 3 1.4 熵的直观解释那么熵的那些描述和解释(混乱程度，不确定性，惊奇程度，不可预测性，信息量等)代表了什么呢？ 如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。 并且当一个罕见的信息到达时，比一个常见的信息有着更多的信息量，因为它排除了别的很多的可能性，告诉了我们一个确切的信息。在天气的例子中，Rainy发生的概率为12.5%，当接收到该信息时，我们减少了87.5%的不确定性(Fine,Cloudy,Snow)；如果接收到Fine(50%)的消息，我们只减少了50%的不确定性。 2. 交叉熵(Cross-Entropy)2.1 交叉熵损失函数？二分类交叉熵？熟悉机器学习的人都知道分类模型中会使用交叉熵作损失函数，也一定对吴恩达的机器学习视频中猫分类器使用的二分类交叉熵印象深刻，但交叉熵究竟是什么？本文的第二部分，让我们一起搞明白交叉熵。字面上看，交叉熵分两部分“交叉”和“熵”，首先回顾下熵的公式吧。 2.2 熵的公式上文中已知一个离散变量 i 的概率分布P(i)，熵的公式可以表示为： Entropy = -\\sum_{i}P(i)\\log_2{P(i)}同理，对于连续变量 x 的概率分布P(x)，熵的公式可以表示为： Entropy = -\\int P(x)\\log_2{P(x)}dx在熵的公式中，对于离散变量和连续变量，我们都是计算了 负的可能性的对数 的期望，代表了该事件理论上的平均最小编码长度，所以熵的公式也可表示如下，公式中的x~P代表我们使用概率分布P来计算期望，熵又可以简写为H： H(P) = Entropy = \\mathbb{E}_{x\\sim{P}}[-\\log{P(x)}]重要的事情再说一遍：“熵是服从某一特定概率分布事件的理论最小平均编码长度”，只要我们知道了任何事件的概率分布，我们就可以计算它的熵；那如果我们不知道事件的概率分布，又想计算熵，该怎么做呢？那我们来对熵做一个估计吧，熵的估计的过程自然而然的引出了交叉熵。 2.3 熵的估计假如我们现在需要预报东京天气，在真实天气发生之前，我们不可能知道天气的概率分布；但为了下文的讨论，我们需要假设：对东京天气做一段时间的观测后，可以得到真实的概率分布P。 在观测之前，我们只有预估的概率分布Q，使用估计得到的概率分布，可以计算估计的熵： EstimatedEntropy =\\mathbb{E}_{x\\sim{Q}}[-\\log{Q(x)}]如果Q是真实的概率分布，根据上述公式，我们已经得到了编码东京天气信息的最小平均长度；然而估计的概率分布为我们的公式引入了两部分的不确定性： 计算期望的概率分布是Q，与真实的概率分布P不同。 计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。 因为估计的概率分布Q影响了上述两个部分(期望和编码长度)，所以得到的结果很可能错得离谱，也因此该结果和真实熵的对比无意义。和香农的目标一样，我们希望编码长度尽可能的短，所以我们需要对比我们的编码长度和理论上的最小编码长度(熵)。假设经过观测后，我们得到了真实概率分布P，在天气预报时，就可以使用P计算平均编码长度，实际编码长度基于Q计算，这个计算结果就是P和Q的交叉熵。这样，实际编码长度和理论最小编码长度就有了对比的意义。 CrossEntropy =\\mathbb{E}_{x\\sim{P}}[-\\log{Q(x)}] \\\\ Entropy = \\mathbb{E}_{x\\sim{P}}[-\\log{P(x)}]2.4 交叉熵 &gt;= 熵交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H(P)。 H(P,Q) =\\mathbb{E}_{x\\sim{P}}[-\\log{Q(x)}]有一点很微妙但很重要：对于期望，我们使用真实概率分布P来计算；对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即Q=P，那么有H(P,Q) = H(P)，否则，H(P,Q) &gt; H(P)。 至此，交叉熵和熵的关系应该比较明确了，下面让我们看看为什么要使用交叉熵作分类损失函数。 2.5 交叉熵作为损失函数假设一个动物照片的数据集中有5种动物，且每张照片中只有一只动物，每张照片的标签都是one-hot编码。 Animal Dog Fox Horse Eagle Squirrel Label [1,0,0,0,0] [0,1,0,0,0] [0,0,1,0,0] [0,0,0,1,0] [0,0,0,0,1] 第一张照片是狗的概率为100%，是其他的动物的概率是0；第二张照片是狐狸的概率是100%，是其他动物的概率是0，其余照片同理；因此可以计算下，每张照片的熵都为0。换句话说，以one-hot编码作为标签的每张照片都有100%的确定度，不像别的描述概率的方式：狗的概率为90%，猫的概率为10%。 假设有两个机器学习模型对第一张照片分别作出了预测：Q1和Q2,而第一张照片的真实标签为[1,0,0,0,0]。 Model Prediction Q1 [0.4, 0.3, 0.05, 0.05, 0.2] Q2 [0.98, 0.01, 0, 0, 0.01] 两个模型预测效果如何呢，可以分别计算下交叉熵： H(P_1,Q_1) =-\\sum_{i}P_1(i)\\log_2{Q_1(i)}\\\\ = -(1\\log{0.4}+0\\log{0.3}+0\\log{0.05}+0\\log{0.05}+0\\log{0.2}) \\approx 0.916\\\\ H(P_1,Q_2) =-\\sum_{i}P_1(i)\\log_2{Q_2(i)}\\\\ = -(1\\log{0.98}+0\\log{0.01}+0\\log{0}+0\\log{0}+0\\log{0.01}) \\approx 0.02\\\\交叉熵对比了模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。因此，训练分类模型时，可以使用交叉熵作为损失函数。 2.5 二分类交叉熵在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性： H(P,Q) =-\\sum_{i=(cat,dog)}P(i)\\log{Q(i)}\\\\ =-P(cat)\\log{Q(cat)} - P(dog)\\log{Q(dog)}又因为： P(cat) = 1 - P(dog)所以交叉熵可以表示为： H(P,Q)=-P(cat)\\log{Q(cat)} - （1-P(cat))\\log{(1-P(cat))}使用如下定义： P = P(cat)\\\\ \\tilde{P} = Q(cat)二分类的交叉熵可以写作如下形式，看起来就熟悉多了。 BinaryCrossEntropy =-P\\log{\\tilde{P}} - （1-P)\\log{(1-\\tilde{P})}","categories":[],"tags":[{"name":"熵","slug":"熵","permalink":"http://yoursite.com/tags/%E7%86%B5/"},{"name":"交叉熵","slug":"交叉熵","permalink":"http://yoursite.com/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"},{"name":"Entropy","slug":"Entropy","permalink":"http://yoursite.com/tags/Entropy/"},{"name":"Cross-Entropy在[Demystifying Entropy](https://medium.com/activating-robotic-minds/demystifying-entropy-f2c3221e2550), Demystifying Cross-Entropy, Demystifying KL Divergence","slug":"Cross-Entropy在-Demystifying-Entropy-https-medium-com-activating-robotic-minds-demystifying-entropy-f2c3221e2550-Demystifying-Cross-Entropy-Demystifying-KL-Divergence","permalink":"http://yoursite.com/tags/Cross-Entropy%E5%9C%A8-Demystifying-Entropy-https-medium-com-activating-robotic-minds-demystifying-entropy-f2c3221e2550-Demystifying-Cross-Entropy-Demystifying-KL-Divergence/"}]},{"title":"每日知识点总结","slug":"每日知识点总结","date":"2020-05-27T12:00:00.000Z","updated":"2020-06-16T10:04:40.325Z","comments":true,"path":"2020/05/27/每日知识点总结/","link":"","permalink":"http://yoursite.com/2020/05/27/%E6%AF%8F%E6%97%A5%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"每日知识点总结 记录每天的学习内容，便于随时巩固。 温故而知新","text":"每日知识点总结 记录每天的学习内容，便于随时巩固。 温故而知新 2020.05.27正态分布+MLE+MAP 假设线性回归误差满足正太分布，则对样本做极大似然估计得到最小二乘法 又假设参数先验分布为正太分布，则对样本做极大后验估计得到Ridge 中心极限定理和正态分布 多个独立统计量的和的平均值满足正态分布 抛掷骰子，出现的点数虽为平均分布，但取N=100统计平均值，平均值满足正态分布 身高为多个因素作用的结果，假设有100个因素，即N=100，每个人的身高都可以看做100因素的平均值(平均影响)，满足正态分布 集成学习(boosting/bagging) boosting：串行，“从错误中汲取经验”，adaboosting的shortcoming是串行过程中上一次错分的样本，gradient boosting的shortcoming是串行过程中上一次的Gradient。总结gradient boosting 比较好的博文：梯度提升、Gradient Boosting bagging：并行，“投票决定”，各个基分类器的训练数据不能相同 2020.05.28全概率、条件概率、乘法公式 一切概率都是条件概率，假设D为全样本空间，则P(A) = P(A∩D)/P(D) 计算概率是要记得用韦恩图想象各样本空间 再看朴素贝叶斯 朴素贝叶斯垃圾邮件分类：其中贝叶斯公式中的每项各自的含义 再看生成式模型和判别式模型 假设X是观测值的集合，Y是预测值的集合 生成式模型对联合概率分布建模P(X,Y)，再计算P(Y|X) = P(X,Y)/P(X)，其中分子就是联合概率，分母可通过边缘概率求出，P(X) = ∑P(X,Y) = ∑P(X|Y)P(Y) 判别模型直接对条件概率进行建模P(Y|X) 激活函数的特点 Sigmoid和Tanh容易过拟合的原因，从函数导数来看，提供的有效激活范围小 ReLU的优点：相对宽的激活边界使得不容易梯度消失、单侧抑制模拟大脑神经元、正反向传播计算复杂度低 ReLU的缺点：lr过大导致神经元死亡，新提出的LeakyReLU进行补救 2020.05.29LATEX 学习 最常用的键’\\\\’，用于将字符串构造成特殊符号，例如：\\gamma \\theta \\mbox \\gamma + \\theta + \\alpha + \\pi + \\mbox for Array相关：\\left(\\begin{array}{cc}\\end{array}\\right) \\left(\\begin{array}{cc} 1&2&3 \\\\ 4&5&6 \\end{array}\\right) 上下标和求和连乘等： X_{1}^{1}\\\\ \\sum_{x=1}^{n}x_i \\\\ \\prod_{x=1}^{n}y_i 概率论：随机试验、随机/基本事件、样本空间等 求概率时，一定要明确总的样本空间和事件的样本空间 随机事件由基本事件组合而成，随机事件的样本空间是总样本空间的子集 C++ ：引用、指针(C++ Primer P50) 引用：引用及别名，只能为对象创建引用，为引用赋值相当于为对象赋值，无法另引用重新绑定到新的值 指针：存放某个对象的地址 相同点：与引用类似，指针也实现了对其他对象的间接访问；需要类型匹配 不同点1：指针本身就是一个对象，允许对指针赋值和拷贝，生命周期内可以先后指向几个不同的对象 不同点2：指针无需在定义是赋值，与其他内置类型一样，在作用域内灭有初始化的指针拥有不确定的值 使用解引用符* 访问引用指向的对象的值 2020.05.30概率论：同济《概率与数理统计》全部内容 贝叶斯公式，通俗解释：假设导致事件A发生的可能的原因有多种B_1,B_2等，事件A已经发生后，贝叶斯负责计算各原因的概率。分母P(A)的全概率公式通过B对其完备分割 边缘概率与全概率：涉及的样本空间不同，边缘概率位于概率计算表的边缘，描述的是多维样本空间关系，对联合概率其中一个维度求和从而忽略该维度；全概率公式描述一维样本空间中各个子样本空间的关系。 const限定符(C++ Primer) 顶层const和底层const，明确const修饰的对象 const int *p (p是一个指针，指向常量） int *const p = &amp;m (p是一个常量指针，指向整形变量，需要初始化) 被const修饰的变量必须初始化 数组名被当做指针使用类的细节 声明类时，成员数据可以声明在成员函数之后，编译器会处理 假设在类内声明成员函数isbn，std::string isbn() const {return bookNo;}返回bookNo时，相当于return this-&gt;bookNo，而const就置于空形参列表之后做限定，相当于const SaleData *const this，限定this指向的是SaleData常量，不允许改变类成员数据的内容。 定义返回 *this的函数：相当于把当前对象返回 ，接收类型是SaleData &amp; 2020.06.01C++：C++ Primer 1-6章节习题数组不能被拷贝；当使用数组名时，使用的是指针。当必须使用数组作为形参进行传递时，void print10(const int (&amp;ia)[10]) GDBT 作为补充特征，提升分类效果2020.06.02C++：C++ Primer 类(p237) 构造函数：没有返回类型。默认构造函数：当类没有声明任何构造函数时 Sales_data() = default：定义了非默认构造函数，该语句指定生成对应的默认构造函数 Sale_data(const std::string &amp;s,unsigned n, double p): bookNo(s),units_sold(n),revenue(p*n) {} 新知识：非默认构造函数 2020.06.04C++：面向对象编程OOP，封装、继承和多态 数据抽象:将类的接口和实现分离+数据封装 继承:类的层次关系:class Hi: public Hello{ 定义 }; 多态：运行时绑定、动态绑定：基类指针或引用指向派生类对象 基类、派生类的定义 基类的两种成员函数，虚函数virtual希望派生类覆盖override virtual只出现在类内的声明部分，不用于类外部的函数定义 基类的静态成员只有唯一的实例，且派生类遵循访问控制规则 派生类中需要覆盖的函数 需要重新声明 派生类构造函数:派生类使用基类的构造函数初始化基类部分 类型转换:基类指针或引用指向派生类对象，将派生类转换成基类，反之不行 对象之间不存在类型转换，派生类部分会被切掉 2020.06.05 电脑图形界面崩掉，开始修复电脑 2020.06.08 Linux 相关：配置apt源：/etc/apt/sources.list 配置linux网卡：/etc/network/interfaces 安装图形界面：ubuntu-desktop/xubuntu-desktop / kubunut-desktop 2020.06.09python编程： 再次使用os.walk():递归的遍历文件：for root,dirs,files in os.walk(‘./‘)，分别为当前根目录，根目录下的文件夹，根目录下的文件 再次使用json：用于解析/编写json 文件 仿射变换：平移+线性变换 线性变换：线性代数中的变换，特点：变换前后，中心位置不变/平行线仍然平行 仿射变换是在线性变换基础上 加上 平移 在opencv中：仿射变换由3个点确定，需要定义3个点变换前后的坐标得到放射变换矩阵 2020.06.10Cmake入门 mkdir build; cd build; cmake ../ cmake命令需要找到CMakeList.txt，并在当前目录生成中间文件，包括Makefile;之后 make &amp;&amp; make install CMakeList.txt文件内容：cmake_minimum_required(VERSION 2.8) project(name)，详见git 项目 cmake_examples Linux系统知识： 从unix到linux，历史演变 Linux系统中，一切皆是文件，文件都有权限，ls -l中的10列，第一列为文件类型，-表示普通文件，d表示目录，后面rwxrwxrwx为用户/组用户/others的读/写/执行权限，chmod 777(二进制数) 进行更改，chown用于更改文件用户 Linux系统目录结构：bin二进制文件,sbin super用户的二进制文件，etc系统配置文件，dev设备，mnt挂在目录，lib库文件，boot系统启动文件，home各用户目录 su(switch user)切换用户，如不指定，则切换root用户，su - 同时切换用户环境 sudo普通用户以管理员身份执行命令，需要输入用户密码 linux系统中文件没有后缀的概念，添加后缀是为了给人区分 执行 二进制可执行文件 时需要指定路径，有三种方法：绝对路径，相对路径和将目录添加到系统环境变量中 2020.06.11《机器学习》周志华 新的角度理解f1score，PR曲线，ROC 曲线和AUC 逻辑回归为什么叫对数几率回归,是对ln(y/1-y)的线性回归 再回顾贝叶斯决策器和朴素贝叶斯分类器 再回顾集成学习中的boosting,bagging,stacking 聚类算法： k-means：目标是将类内的样本聚集在类的中心。随机初始化类中心，计算所有样本距离每个类中心的距离，划入最近的类;重新计算类中心 yolo 中的anchor尺寸由聚类算法得到 2020.06.15回顾Pytorch训练范式1234567891011121314151617181920212223242526272829303132333435363738import torchimport torch.nn as nnimport nn.functional as Fimport torch.utils.data as dataimport torch.optim as optim# define modelclass HelloNet(nn.Module): def __init__(self): self.conv = nn.Conv2d(3,20,3) def forward(self,x): x = self.conv(x) x = F.relu(x)# define DataSetclass HelloData(data.DataSet): def __init__(self): self.datalist = [] def __len__(self): return 200 def __getitem__(self,index): self.datalist[index]model = HelloNet()if os.path.exists('hello_resume.pth'): resume_state_dict = torch.load('hello_resume.pth') model.load_state_dict(resume_state_dict)optimizer = optim.SGD(model.parameters(),lr=0.001)criterion = nn.MSELoss()hello_data = HelloData()dataloader = data.DataLoader(hello_data,batch_size=20)for epoch in epoch_size: for data in dataloader: X, y = data y_hat = model(X) optimizer.zero_grad() loss = criterion(y,y_hat) loss.backward() optimizer.step() torch.save(model.state_dict(),'hello_epoch&#123;&#125;.pth'.format&#123;eopch&#125;) 概率论题：将10个小球随机放入12个盒子，求恰好10个盒子为空的概率？做概率题，最重要的是明确事件是什么，一个样本是什么，全部样本空间是什么，满足条件的样本空间是什么：在该题目中，一个样本就是放完10个小球后，12个盒子的某一种状态，全部样本空间就是放完小球后，12个盒子所有可能的状态：12^10；满足条件的样本空间是恰好10个盒子为空：(C_12^2)*(2^10-2) 回顾熵/交叉熵/KL散度 Entropy：一个事件有多种可能，服从概率分布P，编码每一条信息的最小长度为-log(P(x))，无损地编码该事件的平均最小长度就是熵，同时表征了事件混乱程度/不确定度/惊奇程度/信息量 Entropy = H(P) = \\mathbb{E}_{P}[-\\log(P(x_i))] CrossEntropy：当不知道事件的概率分布，但又想求熵时，就需要对概率分布做估计，假设估计为Q ，H(Q)没有意义；仅使用Q编码信息，仍观测到的真实的概率分布P做期望得到CrossEntropy：H(P,Q)，有H(P,Q)&gt;=H(P) CrossEntropy = H(P,Q) = \\mathbb{E}_{P}[-log(Q(x_i))] KL散度：H(P,Q) - H(P)用于衡量两个分布之间的差异，散度不是严格意义上的距离，因为不满足对称性：即H(P,Q)!=H(Q,P) 2020.06.16线性代数基变换：参考博客基变换,可以明白矩阵左乘的含义：按照博客理解，矩阵上方有一组基(v1,v2…)，矩阵左方有一组基(w1,w2,…),将V中的基转换到W中就是将v表示为w的线性组合：v1 = a11w1 + a12w2+ … ，v1的系数就是矩阵中的第一列,如下矩阵中v1=(2,1),v2=(1,2),w1=(1,0),w2=(0,1)满足v1=2w1+w2,v2=w1+2w2，所以矩阵可以把V中的向量转换到W中，例如将V中的（1,0）转换成为W中的（2,1） \\left[\\begin{array}{cc|r}2&1\\\\1&2 \\end{array}\\right]","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"},{"name":"总结","slug":"总结","permalink":"http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)","slug":"“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)","date":"2020-05-21T13:00:00.000Z","updated":"2020-05-22T01:46:54.553Z","comments":true,"path":"2020/05/21/“直观理解”卷积神经网络(二)：导向反向传播(Guided-Backpropagation)/","link":"","permalink":"http://yoursite.com/2020/05/21/%E2%80%9C%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E2%80%9D%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E4%BA%8C)%EF%BC%9A%E5%AF%BC%E5%90%91%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD(Guided-Backpropagation)/","excerpt":"Striving for Simplicity: The All Convolutional Net 1. 简介上一篇“直观理解”卷积神经网络(一)：反卷积(Deconvnet)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/)提到：一个典型的卷积神经网络主要由卷积操作、非线性(ReLU)、池化操作等构成。很快，Jost Tobias Springenberg 等人开始质疑池化操作的必要性，并提出了不包含池化操作的“全卷积网络”和对应的新可视化方法“导向反向传播”。","text":"Striving for Simplicity: The All Convolutional Net 1. 简介上一篇“直观理解”卷积神经网络(一)：反卷积(Deconvnet)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/)提到：一个典型的卷积神经网络主要由卷积操作、非线性(ReLU)、池化操作等构成。很快，Jost Tobias Springenberg 等人开始质疑池化操作的必要性，并提出了不包含池化操作的“全卷积网络”和对应的新可视化方法“导向反向传播”。 2. 全卷积网络想要质疑并去除池化操作，就需要先明白池化操作的作用。一般认为:池化操作通过成倍的缩小feature maps的尺寸(高、宽)，使得一个特征可覆盖的原图的范围(感受野Receptive Field)成倍的增大，这样高层的特征可以更快地表征图片的宏观信息。而将卷积操作的步幅(stride)设置为大于1的值，理论上可以获得与池化操作相同的效果；论文作者也通过CIFAR-10、CIFAR-100、ImageNet数据集上的不弱的分类性能 进一步说明了实际的可行性(感兴趣的朋友可以在原论文查看具体的网络结构和对比实验)。下图中，卷积操作步幅为2。(图片来源：附录2) 3. 导向反向传播上一节提到的全卷积网络确实在公开数据集上取得了不俗的分类性能，但使用步幅为2的卷积替换掉池化 究竟对网络带来了怎样的影响呢？用我们上一篇文章提到的“反卷积”将 特征可视化看看吧，并没有达到预期效果，具体表现为：低层特征的可视化结果可以反应出特征学习到的内容，但高层特征的可视化结果不是清晰的、可辨认的结构(见下图最左侧)。还记得反卷积网络中反池化操作时利用的池化索引吗，这些索引记录了最大池化时最大值所在的位置。由于不同图片的池化索引各不相同，这样，即使是相同位置的特征通过反卷积网络后，可视化结果也完全不同(池化索引相当于给了不同图片独特的身份标识)。去掉池化层之后，图片的身份标识没有了，自然无法生成与原图对应的可视化结构。 从原理上应该这样理解：低层特征的计算复杂度很小，对应原图的区域很小，仅仅学到了极小的不变性，因此最大激活低层特征的是单一的、固定的模式；而高层特征的计算复杂度很大，对应原图的区域很大，学到了极大地不变性表示，因此不存在单一图片中的单一模式最大化激活高层特征。合理的特征可视化应该针对一张输入图片进行，对丢失了图片身份标识的全卷积网络，使用反卷积网络进行可视化，显然无法取得很好的效果。 还有什么办法可以衡量 图片的哪些部分最大化激活了一个特征吗？有的，那就是通过反向传播，计算该特征值相对于该输入图片的梯度。反向传播和反卷积最大的不同，就是数值通过通过ReLU时计算方式的不同。而导向反向传播可视化方法，上述两者的结合，细节见下图：b中的四行分别表示，经过ReLU时，前向传播、反向传播、反卷积、导向反向传播的不同之处：定义左侧为前向传播的输入列，右侧为前向传播的输出列，前向传播只向后传输入列大于0的值、反向传播只向前传梯度大于0(输入列大于零)的值、反卷积只向前传输出列大于0的值、导向反向传播综合前两者，只向前传输入列和输出列都大于0的值；c中的公式表达更为清晰。 导向反向传播(Guided-Backpropagation)相当于对普通的反向传播加了指导，限制了小于0的梯度的回传，而梯度小于0的部分对应了原图中 削弱了我们想要可视化的特征 的部分，这些部分正是我们不想要的。还记得我们的目标吗，找出最大化激活某个特征的图片部分。有趣的是，导向反向传播在全卷积网络中也表现非常好，上图是模型高层特征(layer6/layer9)的反卷积(deconv)、导向反向传播(Guided-Backpropagation)可视化结果对比。 半小时学会Pytorch Hook一文中通过pytorch模型中各层的hook函数，改变反向传播时对梯度的操作，用少量代码实现了导向反向传播。 附录 Striving for Simplicity: The All Convolutional Net A guide to convolution arithmetic for deep learning","categories":[],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yoursite.com/tags/Neural-Network/"},{"name":"Visualization","slug":"Visualization","permalink":"http://yoursite.com/tags/Visualization/"},{"name":"神经网络可视化","slug":"神经网络可视化","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"导向反向传播","slug":"导向反向传播","permalink":"http://yoursite.com/tags/%E5%AF%BC%E5%90%91%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"}]},{"title":"“直观理解”卷积神经网络(一)：反卷积(Deconvnet)","slug":"“直观理解”卷积神经网络(一)：反卷积(Deconvnet)","date":"2020-05-14T10:00:00.000Z","updated":"2020-05-22T01:47:09.285Z","comments":true,"path":"2020/05/14/“直观理解”卷积神经网络(一)：反卷积(Deconvnet)/","link":"","permalink":"http://yoursite.com/2020/05/14/%E2%80%9C%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E2%80%9D%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E4%B8%80)%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF(Deconvnet)/","excerpt":"“Visualizing and Understanding Convolutional Networks” — Matthew D.Zeiler 1. 简介时间倒退回2014年，大量基于卷积神经网络的模型在ImageNet数据集上的分类性能有了巨大的提升，其中翘楚当属AlexNet，凭借16.4%的准确率拿下2012年冠军，远远领先第二名的26%。但大多数人并没有完全理解这些模型表现为何如此之好，模型的改善也就无从谈起。对于模型的直观理解迫在眉睫，而本文提到的反卷积技术(Matthew D.Zeiler)就是该领域的开山之作：该技术尝试在输入图片的像素空间中找出最大化激活某一特征的像素，实现了对应特征的可视化，其中反卷积的过程就是寻找像素的过程。借助反卷积，作者也探索了“模型训练过程中，特征的演化”、“分类结果相对于图片遮挡部位的敏感性”、“特征不变性”和“图片相关性”等问题。","text":"“Visualizing and Understanding Convolutional Networks” — Matthew D.Zeiler 1. 简介时间倒退回2014年，大量基于卷积神经网络的模型在ImageNet数据集上的分类性能有了巨大的提升，其中翘楚当属AlexNet，凭借16.4%的准确率拿下2012年冠军，远远领先第二名的26%。但大多数人并没有完全理解这些模型表现为何如此之好，模型的改善也就无从谈起。对于模型的直观理解迫在眉睫，而本文提到的反卷积技术(Matthew D.Zeiler)就是该领域的开山之作：该技术尝试在输入图片的像素空间中找出最大化激活某一特征的像素，实现了对应特征的可视化，其中反卷积的过程就是寻找像素的过程。借助反卷积，作者也探索了“模型训练过程中，特征的演化”、“分类结果相对于图片遮挡部位的敏感性”、“特征不变性”和“图片相关性”等问题。 2. 卷积网络和反卷积网络2.1 卷积网络一个典型的卷积神经网络模型由一系列层(layer)构成，将一张RGB图像x转化为一个向量y_hat，向量中的C个值对应了C类的可能性。当时，卷积网络还很单纯(没有shortcut connection、没有DW、PW Conv等)，每一层都由如下构成： 卷积操作：采用一组训练得到的卷积核对上一层的输出作卷积。 增加非线性：让卷积结果通过非线性函数，例如relu(x) = max(x, 0)。 池化操作[可选]：取相邻元素的最大值(MaxPooling)或平均值(AveragePooling)，进行下采样；MaxPooling中池化索引记录了最大值对应的位置。最后添加全连接层和Softmax分类器，输出C个类别的可能性。 2.2 反卷积网络为了理解卷积网络，需要理解中间层的特征值，反卷积网络提供了办法。反卷积网络的每一层都可以看做卷积网络中对应层的逆过程，它们拥有相同的卷积核和池化索引，因此反卷积将特征值逆映射回了输入图片的像素空间，借此说明图片中的哪些像素参与激活了该特征值。下图将卷积网络和反卷积网络的过程合并，展示了两者各层之间的关系，且两者在整体上互为逆过程：首先，卷积网络将一张图片作为输入，计算得到各层的特征表示；为了验证某层一个具体的特征值，我们将该层特征值之外的所有值置零后，将其作为反卷积网络的输入，经过反卷积网络每一层的操作，该特征值被映射回了输入图片的像素空间. 反卷积网络由如下部分构成： 反池化操作[对应卷积网络池化操作]：理论上，卷积网络中的最大池化操作是不可逆的，但我们可以通过池化索引进行近似可逆。下图(图片来源：附录3)是反池化过程： 矫正[对应卷积网络ReLU操作]：卷积网络中采用ReLU确保特征值非负，为了确保正逆过程的一致性，我们将反卷积网络每一层的重构特征也通过ReLU得到非负值。 反卷积操作[对应卷积网络卷积操作]：卷积操作是低效操作，主流神经网络框架都是通过im2col+矩阵乘法实现卷积，以空间换效率。输入中每个卷积窗口内的元素被拉直成为单独一列，这样输入就被转换为了H_out * W_out列的矩阵(Columns)，im2col由此得名；将卷积核也拉成一列后(Kernel)，左乘输入矩阵，得到卷积结果(Output)。im2col和矩阵乘法见如下两图(图片来源：附录1)。 本文中提到的反卷积操作其实是转置卷积，神经网络框架借助转置卷积实现梯度的反向传播：如下两图(图片来源：附录1)：将卷积核矩阵转置(Weight_T)后，左乘梯度输出(GradOutput)，得到梯度列矩阵(GradColumns)，通过col2im还原到输入大小，便得到了相对于输入的梯度(GradInput)。 梯度的反向传播一般用于卷积网络的训练过程，通过梯度更新权重和偏正取值；而本文转置卷积不是用于梯度反向传播，操作对象不是梯度而是特征值。 3.卷积网络可视化使用上述的理论框架，我们就可以通过反卷积网络来“直观理解”特征值。 3.1 特征可视化下图展示了训练完成后，各层特征值的可视化结果。对于某一层某一个特定位置的特征，选取了9个最大的激活值(由9张输入图片经过卷积网络生成)，并分别将其映射回输入图片的像素空间，可视化结果的旁边是特征对应的可视野范围的输入图片截图。从Layer1 Feature Maps中选了9个位置的特征值，每个特征值选了最能激活该值的9张图片；从Layer2 Feature Maps中选了16个位置的特征值，每个特征值选了最能激活该值的9张图片。几点启示： 原图的截图比可视化结果的多样性更大，因为后者仅仅关注了截图中有判别能力的结构。举例来说：层5第1行第2列的9张输入原图差异很大，但同一特征值的可视化结果都关注了背景中的草地。 每层特征的可视化结构展示了各层的特点。层2展示了物体的轮廓和颜色的组合，每个特征的可视化结果大致相同(较小的不变性)；层3有了更复杂的不变性，主要是相似的纹理(1行1列的网格特征、2行4列的文本特征)；层4的不同特征对应的可视化结果有显著的差别，开始展现类与类之间的差异(1行1列都是狗狗的脸，4行2列都是鸟的腿)；层5的不同特征对应的可视化结果对应了类别间的物体的整体差异(不再是局部)(1行1列键盘，4行的狗)。因为可视野相应的变大了。 每个特征值对应的9个可视化结果有很强的关联性。 层次越高，不变性越强。 可视化结果都是原图中辨识度最强的部分的夸张展现。 3.2 训练过程中特征的演化在每一层随机选取6个特征，在训练过程中的第[1,2,5,10,20,30,40,64]个epoch时，将样本中最强激活的特征映射回输入图片像素空间。可以看出：底层特征很快可以趋于稳定，但高层特征需要更多次的迭代才能收敛，证明了迭代次数的必要性。 3.3 特征不变性下图第1列是5张示例图片的三种变换，分别为平移、缩放、旋转；列2和列3是层1和层7变换图和原图间的特征向量的欧几里得距离；列4是对应图片分类可能性的变化。总体来说：微小的变换对于低层的特征有显著地影响，而层越高，平移和缩放对结果的影响越小；但卷积网络无法对旋转操作产生不变性，除非物体有很强的不对称性。 3.4 遮挡敏感性当模型的分类性能提高时，一个自然而然的想法就是：分类器究竟基于什么信息做出了判断？是基于图片中的物体，还是图片中与物体无关的上下文信息。基于错误的信息做出判断就会出现著名的“坦克分类问题”。下图采用灰色矩形遮挡输入图片的不同部分，并将遮挡后的图片输入网络得到分类器的输出，1~3行分别是博美犬、车轮和阿富汗猎犬；a列是原图，b列是遮挡不同部位后得到的第五层的热力图(选取了不遮挡图片最大激活值所在的通道)，c列是不遮挡原图第5层最大激活值的可视化结果，其余3张来自别的输入图片，d列是遮挡不同部位后，正确分类可能性的热力图，e列是遮挡不同部位后，最可能的分类结果。总体来说： b列说明：当遮挡了可视化结果对应的原图部位时，对应的特征值会急剧下降； d列说明：遮挡博美犬脸的部位、车轮部位、阿富汗犬部位，正确分类的可能性急剧下降。 附录：1.PyTorch源码浅析(3)：NN 2Visualizing and Understanding Convolutional Networks 3.反卷积(Deconvolution)、上采样(UNSampling)与上池化(UnPooling)","categories":[],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yoursite.com/tags/Neural-Network/"},{"name":"Visualization","slug":"Visualization","permalink":"http://yoursite.com/tags/Visualization/"},{"name":"反卷积","slug":"反卷积","permalink":"http://yoursite.com/tags/%E5%8F%8D%E5%8D%B7%E7%A7%AF/"},{"name":"神经网络可视化","slug":"神经网络可视化","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"}]}]